// @generated by Cursor AI (Claude) ‚Äî verified by Kevin

import { useState, useEffect, useCallback, useRef } from 'react';

/**
 * Custom Hook pour la reconnaissance vocale (Speech-to-Text)
 * 
 * Utilise l'API Web Speech API SpeechRecognition pour convertir la parole en texte.
 * 
 * Fonctionnalit√©s :
 * - D√©marrer/arr√™ter l'√©coute
 * - Capturer le texte transcrit
 * - G√©rer les permissions microphone
 * - G√©rer les erreurs (permission refus√©e, API non support√©e)
 * - Support multilingue (FR, Wolof)
 * 
 * @param {Object} [options] - Options du hook
 * @param {string} [options.language='fr-FR'] - Langue de reconnaissance
 * @param {boolean} [options.continuous=false] - Continuer l'√©coute apr√®s une pause
 * @param {boolean} [options.interimResults=true] - Afficher les r√©sultats interm√©diaires
 * @param {number} [options.maxAlternatives=1] - Nombre d'alternatives de transcription
 * 
 * @returns {Object} √âtat et fonctions de reconnaissance vocale
 * @property {string} transcript - Texte transcrit
 * @property {boolean} isListening - true si l'√©coute est en cours
 * @property {boolean} isSupported - true si l'API est support√©e
 * @property {Object|null} error - Erreur √©ventuelle
 * @property {function} startListening - D√©marrer l'√©coute
 * @property {function} stopListening - Arr√™ter l'√©coute
 * @property {function} abort - Abandonner l'√©coute
 * @property {function} clearTranscript - Effacer le transcript
 * 
 * @example
 * const { startListening, stopListening, transcript, isListening, error } = useSpeechRecognition();
 * 
 * // D√©marrer l'√©coute
 * startListening();
 * 
 * // Arr√™ter et r√©cup√©rer le transcript
 * stopListening();
 * console.log(transcript); // "Amadou"
 */
export function useSpeechRecognition(options = {}) {
  const {
    language = 'fr-FR',
    continuous = false,
    interimResults = true,
    maxAlternatives = 1,
  } = options;

  const [transcript, setTranscript] = useState('');
  const [isListening, setIsListening] = useState(false);
  const [isSupported, setIsSupported] = useState(false);
  const [error, setError] = useState(null);

  const recognitionRef = useRef(null);
  const finalTranscriptRef = useRef('');

  /**
   * V√©rifier si l'API SpeechRecognition est support√©e
   */
  useEffect(() => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    
    if (SpeechRecognition) {
      try {
        const recognition = new SpeechRecognition();
        recognitionRef.current = recognition;
        setIsSupported(true);
        setError(null);
        console.log('‚úÖ SpeechRecognition support√©');
      } catch (err) {
        setIsSupported(false);
        setError({
          code: 'INIT_ERROR',
          message: 'Impossible d\'initialiser la reconnaissance vocale',
          originalError: err,
        });
        console.error('‚ùå Erreur initialisation SpeechRecognition:', err);
      }
    } else {
      setIsSupported(false);
      setError({
        code: 'NOT_SUPPORTED',
        message: 'La reconnaissance vocale n\'est pas support√©e par ce navigateur',
      });
      console.warn('‚ö†Ô∏è SpeechRecognition non support√©');
    }
  }, []);

  /**
   * Configurer la reconnaissance vocale
   */
  useEffect(() => {
    if (!recognitionRef.current) return;

    const recognition = recognitionRef.current;

    // Configurer les options
    recognition.lang = language;
    recognition.continuous = continuous;
    recognition.interimResults = interimResults;
    recognition.maxAlternatives = maxAlternatives;

    // G√©rer les √©v√©nements
    recognition.onstart = () => {
      setIsListening(true);
      setError(null);
      finalTranscriptRef.current = '';
      setTranscript('');
      console.log('üé§ √âcoute d√©marr√©e');
    };

    recognition.onresult = (event) => {
      let interimTranscript = '';
      let finalTranscript = finalTranscriptRef.current;

      // Parcourir tous les r√©sultats
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      finalTranscriptRef.current = finalTranscript;
      
      // Afficher le transcript final + interim
      if (interimResults) {
        setTranscript(finalTranscript + interimTranscript);
      } else {
        setTranscript(finalTranscript);
      }
    };

    recognition.onerror = (event) => {
      setIsListening(false);
      
      let errorMessage = 'Erreur lors de la reconnaissance vocale';
      let errorCode = 'RECOGNITION_ERROR';

      switch (event.error) {
        case 'no-speech':
          errorMessage = 'Aucune parole d√©tect√©e. Veuillez r√©essayer.';
          errorCode = 'NO_SPEECH';
          break;
        case 'audio-capture':
          errorMessage = 'Aucun microphone d√©tect√©. V√©rifiez votre mat√©riel.';
          errorCode = 'NO_MICROPHONE';
          break;
        case 'not-allowed':
          errorMessage = 'L\'acc√®s au microphone a √©t√© refus√©. Veuillez autoriser l\'acc√®s dans les param√®tres.';
          errorCode = 'PERMISSION_DENIED';
          break;
        case 'network':
          errorMessage = 'Erreur r√©seau. V√©rifiez votre connexion.';
          errorCode = 'NETWORK_ERROR';
          break;
        case 'aborted':
          // Ignorer les erreurs d'abandon (arr√™t manuel)
          return;
        default:
          errorMessage = event.error || errorMessage;
      }

      setError({
        code: errorCode,
        message: errorMessage,
        originalError: event,
      });

      console.error('‚ùå Erreur reconnaissance vocale:', event.error);
    };

    recognition.onend = () => {
      setIsListening(false);
      console.log('‚èπÔ∏è √âcoute termin√©e');
    };

  }, [language, continuous, interimResults, maxAlternatives]);

  /**
   * Nettoyer lors du d√©montage
   */
  useEffect(() => {
    return () => {
      if (recognitionRef.current && isListening) {
        recognitionRef.current.abort();
      }
    };
  }, [isListening]);

  /**
   * D√©marrer l'√©coute
   * 
   * @param {Object} [overrides] - Options √† surcharger
   * @returns {Promise<void>}
   */
  const startListening = useCallback(async (overrides = {}) => {
    if (!isSupported || !recognitionRef.current) {
      setError({
        code: 'NOT_SUPPORTED',
        message: 'La reconnaissance vocale n\'est pas support√©e',
      });
      return;
    }

    try {
      // V√©rifier la permission microphone
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        // Lib√©rer imm√©diatement (on a juste besoin de la permission)
        stream.getTracks().forEach(track => track.stop());
      } catch (permError) {
        if (permError.name === 'NotAllowedError' || permError.name === 'PermissionDeniedError') {
          setError({
            code: 'PERMISSION_DENIED',
            message: 'L\'acc√®s au microphone a √©t√© refus√©. Veuillez autoriser l\'acc√®s dans les param√®tres de votre navigateur.',
            originalError: permError,
          });
          return;
        }
        throw permError;
      }

      // Appliquer les overrides si fournis
      if (overrides.language) {
        recognitionRef.current.lang = overrides.language;
      }

      // D√©marrer la reconnaissance
      recognitionRef.current.start();
      console.log('üé§ D√©marrage de l\'√©coute...');

    } catch (err) {
      setIsListening(false);
      setError({
        code: 'UNEXPECTED_ERROR',
        message: err.message || 'Une erreur inattendue est survenue',
        originalError: err,
      });
      console.error('‚ùå Erreur inattendue startListening:', err);
    }
  }, [isSupported]);

  /**
   * Arr√™ter l'√©coute
   */
  const stopListening = useCallback(() => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop();
      console.log('‚èπÔ∏è Arr√™t de l\'√©coute...');
    }
  }, [isListening]);

  /**
   * Abandonner l'√©coute (arr√™t imm√©diat)
   */
  const abort = useCallback(() => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.abort();
      setIsListening(false);
      console.log('üõë √âcoute abandonn√©e');
    }
  }, [isListening]);

  /**
   * Effacer le transcript
   */
  const clearTranscript = useCallback(() => {
    setTranscript('');
    finalTranscriptRef.current = '';
  }, []);

  return {
    transcript,
    isListening,
    isSupported,
    error,
    startListening,
    stopListening,
    abort,
    clearTranscript,
  };
}

export default useSpeechRecognition;

